{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial will introduce you to Keras - a high level neural networks API written in Python that was developed with a focus on enabling fast experimentation. In their own words, \n",
    "\n",
    "_\"Being able to go from idea to result with the least possible delay is key to doing good research.\"_\n",
    "\n",
    "Keras is library that allows for fast prototyping for both convolutional and recurrent networks through user friendliness, modularity and extensibility. As we walk through the basics of Keras and the sequential model, aided by code examples on the IMDB dataset (conveniently also included in Keras) you will see these perks actively in play.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Content\n",
    "\n",
    "- [Installing Keras and its dependencies](#Installing-Keras-and-its-dependencies)\n",
    "- [Key features of Keras](#Key-features-of-Keras)\n",
    "- [Basics](#Basics)\n",
    "    - [Neural Networks](#Neural-Networks)\n",
    "    - [Models](#Models)\n",
    "    - [Layers](#Layers)\n",
    "- [Keras Sequential Model](#Keras-Sequential-Model)\n",
    "    - [Initializing models, layers and datasets](#Initializing-models,-layers-and-datasets)\n",
    "    - [Visualizing models and layers](#Visualizing-models-and-layers)\n",
    "    - [Parameters: Input shape](#Parameters:-Input-shape)\n",
    "    - [Model 1](#Model-1)\n",
    "    - [Model 2](#Model-2)\n",
    "    - [Model 3](#Model-3)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Summary and References](#Summary-and-References)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Keras and its dependencies\n",
    "\n",
    "Installing [Keras](https://keras.io) is as easy is typing a couple of commands into your shell. Keras can work with any one of [TensorFlow](https://tensorflow.org), [CNTK](https://github.com/microsoft/CNTK) or [Theano](http://deeplearning.net/software/theano/). In this tutorial we will be using TensorFlow as the backend for Keras. Both of these also require [NumPy](https://numpy.org/) which you should already be well acquainted with.\n",
    "[Pip](https://pip.pypa.io/en/stable/) accepts a space seperated list of things to install, so everything we need for this tutorial can be installed by running this next block right in jupyter (only uncomment and run it if you need to install keras for the first time or have a corrupted version of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install --user --upgrade --force-reinstall keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can run this in the same shell you launch jupyter from:\n",
    "\n",
    "$ pip3 install numpy tensorflow keras\n",
    "\n",
    "We need to install tensorflow here because keras builds on top of tensorflow and essentially serves as a wrapper for tensorflow to make it more easily accessible and extensible.\n",
    "\n",
    "After the installation finishes, make sure the following commands work for you and you are able to import all the necessities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #to hide annoying future warnings from tensorflow/numpy\n",
    "\n",
    "import numpy\n",
    "import json\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key features of Keras\n",
    "\n",
    "Keras sells itself based on 4 key features. For data scientists, these features are exceptionally useful in cases where they would want to analyze a data through the lens of a machine learning model (like a neural net) \n",
    "\n",
    "__User friendliness:__ Keras follows the best practices for reducing cognitive load on the user with consistent and simple APIs and minimizes the number of actions required for common use cases.\n",
    "\n",
    "__Modularity:__ A model in Keras is understood as a sequence of configurable modules. These can be plugged in together with as few restrictions as possible.\n",
    "\n",
    "__Easy Extensibility:__ New modules are simple to add yourself which makes it ideal for advanced research.\n",
    "\n",
    "__Python:__ The models in Keras are described in compact Python code which is easy to debug and extend upon.\n",
    "\n",
    "Throughout this tutorial we will be highlighting these key features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "A neural network is a set of algorithms tied together in \"layers.\" A model of a neural network might often look like something in the image below. Each node in a layer can take as input any linear combination of the values of the nodes in the layer before it.\n",
    "\n",
    "Keras takes this general idea and makes it easier to understand and visualize by making each model consist of explicit layers with explicit algorithms assigned to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/Kc50L.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://i.stack.imgur.com/Kc50L.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "There are two main types of models in Keras - the Sequential model and the Model class used with the functional API.\n",
    "\n",
    "The common attributes of a model are:\n",
    "- __model.layers__ is a flattened list of the layers that comprise the model\n",
    "- __model.inputs__ is a list of input tensors to the model\n",
    "- __model.outputs__ is a list of output tensors\n",
    "- __model.summary()__ prints a summary of the model\n",
    "\n",
    "We will see examples of these once we understand what layers are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "A layer in Keras corresponds to the common conception of a layer in a neural network.\n",
    "\n",
    "It has the following attributes:\n",
    "- __layer.get_weights()__ gets the weights of the layer as a list of numpy ndarrays\n",
    "- __layer.set_weights()__ sets the weights of the layer from a list of numpy ndarray\n",
    "- __layer.get_config()__ returns a dictionary with the configuration of the layer\n",
    "\n",
    "We now proceed to see examples of these functions and how to set up a model in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Sequential Model\n",
    "\n",
    "### Initializing models, layers and datasets\n",
    "\n",
    "We can import models and layers from keras as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the IMDB dataset that comes packaged with Keras. This is a dataset of 25,000 movie reviews from IMDB labeled by sentiment (postive or negative). The reviews are processed and encoded as a sequence of word indexes. These are also indexed by overall frequency in the dataset which allows for quick filtering operations like \"only consider the top 5,000 most frequent words but ignore the top 20 most common.\" A query like this would make sense because intuitively words outside this range probably don't affect the sentiment. The most common words would just be connectors and propositions whereas words the aren't frequent at all are probably nouns or other non-indicators of sentiment.\n",
    "\n",
    "This is an interesting dataset to demonstrate Keras' power with, since this would seem like something complicated to do from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000, skip_top=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x's here is a ndarray of samples. Each sample consists of a list of words indicated by the word frequency rank index which tells us what words were used in each review. The y's here are ndarray of labels, each label being 0 or 1 to indicate whether a review was positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize a sequential model in Keras as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  <keras.engine.sequential.Sequential object at 0x7f5a10244a58>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "print(\"model : \", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a Sequential object. We can then interact with this object by adding layers to it as such.\n",
    "A dense layer can be initialized with a single argument - the number of nodes you want the layer to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 50\n",
    "layer1 = Dense(num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add this layer to the model with the add() method. If we take a look at the model's layers now we'll see that the Dense layer has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers :  [<keras.layers.core.Dense object at 0x7f5a10244d30>]\n"
     ]
    }
   ],
   "source": [
    "model.add(layer1)\n",
    "print(\"model.layers : \", model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and add a relu activation layer on top of the Dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers :  [<keras.layers.core.Dense object at 0x7f5a10244d30>, <keras.layers.core.Activation object at 0x7f5a0755c470>]\n"
     ]
    }
   ],
   "source": [
    "layer2 = Activation('relu')\n",
    "model.add(layer2)\n",
    "print(\"model.layers : \", model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras can make this even easier by letting you directly initialize a model with layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers :  [<keras.layers.core.Dense object at 0x7f5a0755c780>, <keras.layers.core.Activation object at 0x7f5a0755c908>, <keras.layers.core.Dense object at 0x7f5a0755c940>, <keras.layers.core.Activation object at 0x7f5a0755cac8>]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, input_shape=(400,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "])\n",
    "print(\"model.layers : \", model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most common practical scenarios, you want to follow dense layers with activation layers like we did above since the default linear activation isn't very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing models and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_config() method gives us a good insight into what defines a layer and what can be modified in a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"activation\": \"linear\",\n",
      "    \"activity_regularizer\": null,\n",
      "    \"bias_constraint\": null,\n",
      "    \"bias_initializer\": {\n",
      "        \"class_name\": \"Zeros\",\n",
      "        \"config\": {}\n",
      "    },\n",
      "    \"bias_regularizer\": null,\n",
      "    \"dtype\": \"float32\",\n",
      "    \"kernel_constraint\": null,\n",
      "    \"kernel_initializer\": {\n",
      "        \"class_name\": \"VarianceScaling\",\n",
      "        \"config\": {\n",
      "            \"distribution\": \"uniform\",\n",
      "            \"mode\": \"fan_avg\",\n",
      "            \"scale\": 1.0,\n",
      "            \"seed\": null\n",
      "        }\n",
      "    },\n",
      "    \"kernel_regularizer\": null,\n",
      "    \"name\": \"dense_1\",\n",
      "    \"trainable\": true,\n",
      "    \"units\": 50,\n",
      "    \"use_bias\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(layer1.get_config(), indent=4, sort_keys=True)) #json dumps to pretty print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way of visualising a model is the summary() method. We use json.loads() and json.dumps() to pretty print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 32)                12832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 13,162\n",
      "Trainable params: 13,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters: Input Shape\n",
    "\n",
    "Before we can start using the model on our data, we need to reinstantiate it because we missed one key thing - the input shape parameter. When creating a model in Keras, adding an input shape to the first layer suffices as it calculates it for the remaining layers implicitly. In the case of our imdb data, lets see what the input shape is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "y_train shape: (25000,)\n",
      "y_test shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=400) #only let each sample be 400 long\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=400) #only let each sample be 400 long\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After have padded the sequences, we see the sample shapes are (25000, 400), meaning 25000 samples with 400 features in each. Now we can use this information initiate a new model that is actually capable of running on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "\n",
    "Let's start by making a simple sequential model. Just add a couple of relu activated layers and squash the result into a sigmoid. We can use the summary() method to get a very nice and clean outlook on all the layers in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 160,801\n",
      "Trainable params: 160,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(200, activation = \"relu\", input_shape=(400,)))\n",
    "model_1.add(Dense(200, activation = \"relu\"))\n",
    "model_1.add(Dense(200, activation = \"relu\"))\n",
    "model_1.add(Dense(1, activation = \"sigmoid\"))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and fit this model with a mean squared optimizer (\"rmsprop\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zirus23/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/zirus23/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 1s 57us/step - loss: 11.2203 - accuracy: 0.5080 - val_loss: 0.7643 - val_accuracy: 0.5013\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 1s 51us/step - loss: 0.7221 - accuracy: 0.4972 - val_loss: 0.7015 - val_accuracy: 0.4998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5a02b621d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.compile(\n",
    " optimizer = \"rmsprop\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_1.fit(\n",
    " x_train, y_train,\n",
    " epochs= 2,\n",
    " batch_size = 50,\n",
    " validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 51% accuracy on the test data, that was a pretty bad result. Let's try it with a different popular optimizer, adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 2s 61us/step - loss: 0.7103 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 2s 68us/step - loss: 0.6964 - accuracy: 0.4984 - val_loss: 0.6932 - val_accuracy: 0.4999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5a02c8bba8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_1.fit(\n",
    " x_train, y_train,\n",
    " epochs= 2,\n",
    " batch_size = 50,\n",
    " validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output accuracy is still barely better than randomly guessing at little over 50%. At this point maybe we should realize that our model was bad to begin with and we need to redo it. This would normally be a fairly disheartening realization after spending hours writing this model. With Keras, it took us all of 10 minutes to make this model. We have no hesitation completely dropping the model and starting from scratch on a new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "\n",
    "Some preliminary research has led us to the intuition that a bidirectional LSTM might be good for predicting on this kind of dataset. We're not sure, however and don't want to spend all the time writing such a complicated algorithm. With Keras, this is again just a matter of 5 lines of code to construct a model. We saw that the training accuracy was decent last time but the generalization was poor, so let's throw in a dropout layer too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 64)           320000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 386,177\n",
      "Trainable params: 386,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(5000, #max features\n",
    "                      64, #number of nodes in embedding layer\n",
    "                      input_length=400)) #number of features in each input sample\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Bidirectional(LSTM(64)))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's compile this model and test its fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 240s 10ms/step - loss: 0.4753 - accuracy: 0.7670 - val_loss: 0.3834 - val_accuracy: 0.8435\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 265s 11ms/step - loss: 0.2958 - accuracy: 0.8804 - val_loss: 0.3534 - val_accuracy: 0.8448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5a0755c3c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(x_train, numpy.array(y_train),\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          validation_data=[x_test, numpy.array(y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a much better accuracy this time at 85% (which varies a little every run) but we want to do even better. Again, instead of fine tuning these parameters, let's try a completely different model because we can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "\n",
    "By now you get the idea. We can add even more layers with more nodes and combine them in more complex ways with barely any additional effort. To top it all, the code is still self-explanatory because of the style keras enforces with its variable names and we can clearly see what layers the model consists of, what the input is and what form the output of the compile is. Since this is our final attempt, let's get serious and use a convolutional layer with max pooling (again with an embedding layer and a couple of dense relu activated layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               50200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 378,351\n",
      "Trainable params: 378,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "\n",
    "# Input Layers\n",
    "model_3.add(Embedding(5000,               # max features\n",
    "                      50,                 # number of nodes in embedding layer\n",
    "                      input_length=400))  # equal to maxlen\n",
    "\n",
    "# convolutional 1 dimensional layer\n",
    "model_3.add(Conv1D(250,                   # number of nodes in conv layer\n",
    "                   3,                     # kernel size\n",
    "                   padding='valid',\n",
    "                   activation='relu'))\n",
    "model_3.add(GlobalMaxPooling1D())         # use max pooling for the convolutional layer\n",
    "\n",
    "#HIDDEN LAYER\n",
    "model_3.add(Dense(200))                   # number of nodes in hidden layer\n",
    "model_3.add(Dropout(0.2))                 # dropout to avoid overfitting\n",
    "model_3.add(Activation('relu'))           # simple relu activation\n",
    "\n",
    "model_3.add(Dense(200))\n",
    "model_3.add(Dropout(0.2))\n",
    "model_3.add(Activation('relu'))\n",
    "\n",
    "#OUTPUT LAYER\n",
    "model_3.add(Dense(1))                     # sigmoid activated layer to squash the output\n",
    "model_3.add(Activation('sigmoid'))\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good idea what the model and its layers look like, lets check its performance with an adam optimizer and cross entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 56s 2ms/step - loss: 0.3883 - accuracy: 0.8118 - val_loss: 0.2956 - val_accuracy: 0.8734\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 56s 2ms/step - loss: 0.2102 - accuracy: 0.9178 - val_loss: 0.2679 - val_accuracy: 0.8893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5a07230828>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_3.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After just 2 epochs and under 100s seconds of runtime (on a low end CPU) we have 88.92% accuracy. What's even more astonishing than how quickly this model got good is how easy it was to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We saw three different models operate on the IMDB dataset. The output varied significantly in accuracy and we used completely different kinds of layers in the three models. When a model didn't work, we had no probelm switching to a new one. Making these three entirely different models using even tensorflow would have taken significantly longer and would have been a lot more code. We probably would never even have tried the third model and just kept trying to adjust the second model's parameters because who would want to write hours to code just to try a different approach that might or not be slightly better? \n",
    "Keras made the process of experimenting with different models significantly faster and more fun. This enables data scientists like us to spend our time thinking about the high level ideas of which models to use with what types of layers instead of debugging implementations that others have already written hundreds of times. We can very quickly test the hypothetical models that we imagine with Keras and quickly improve upon them. Quoting again what was said at the start of this tutorial:\n",
    "\n",
    "_\"Being able to go from idea to result with the least possible delay is key to doing good research.\"_\n",
    "\n",
    "This is the power of Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and References\n",
    "\n",
    "If you're not convinced to use Keras from this tutorial, let the developers make a case for themselves\n",
    "\n",
    "https://keras.io/why-use-keras/\n",
    "\n",
    "For a more complete guide on the sequential model, go here.\n",
    "\n",
    "https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "To define more complex models without having to extend the library, Keras has the functional API. It works very similarly to the Sequential model we covered here but removes the constraint of being sequential.\n",
    "\n",
    "https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "Keras supports a huge assortment of layers that are commonly used machine learning techniques. \n",
    "To learn more about Keras's layers, go here.\n",
    "\n",
    "https://keras.io/layers/about-keras-layers/\n",
    "\n",
    "However, the single best resource is learning by doing. Here dozens of simple Keras based applications with open source code that you can modify and play with.\n",
    "\n",
    "https://github.com/keras-team/keras/tree/master/examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
